{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = \"4e291d9c-27e4-438e-b424-57a7ea0ba08a\"\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "groq_api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relative path to the JSON file\n",
    "relative_path = os.path.join('.', 'final_results.json')\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Load the JSON file into a DataFrame\n",
    "data = pd.read_json(relative_path)\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=row['article_text'],\n",
    "        metadata={\n",
    "            'title': row['title'],\n",
    "        }\n",
    "    )\n",
    "    for _, row in data.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the split documents\n",
    "split_documents = []\n",
    "\n",
    "# Split the article texts and create documents\n",
    "for _, row in tqdm(data.iterrows(), total=data.shape[0], desc=\"Splitting documents\"):\n",
    "    article_text = row['article_text']\n",
    "    title = row['title'] if pd.notnull(row['title']) else \"\"\n",
    "    chunks = text_splitter.split_text(article_text)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        split_documents.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={'title': title}\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Connect to the Pinecone index\n",
    "index_name = \"elrond-index\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = PineconeVectorStore.from_documents(split_documents, embeddings, pinecone_api_key=pinecone_api_key, index_name=index_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is U-boot?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, groq_api_key=groq_api_key, model=\"mixtral-8x7b-32768\")\n",
    "prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks specifically on Embedded Linux and its components like U-boot, Linux kernel, hardware and software stack. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Answer the questions with a maximum of 512 words.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide for Agentic Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import SerperDevTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# You can choose to use a local model through Ollama for example. See https://docs.crewai.com/how-to/LLM-Connections/ for more information.\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = 'gpt-4-0125-preview'\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY_OPENAI\n",
    "\n",
    "search_tool = SerperDevTool()\n",
    "\n",
    "# Define your agents with roles and goals\n",
    "Engineer_Reviewer = Agent(\n",
    "  role=\"Embedded Linux Software engineer evaluating AI responses\",\n",
    "  goal=\"\"\"Give a score from 0 to 1 to the AI answer, defining how it compares to a human answer in terms of clarity, completeness, accuracy, and relevance to embedded Linux topics. \n",
    "  The score 0 means that the AI answer is very different or unrelated compared to the human answer. \n",
    "  The score 1 means that the AI answer fully accomplishes the goal and is very similar to the human answer, exhibiting attributes like precision, technical correctness, and \n",
    "  effective communication of concepts pertinent to Embedded Linux development.\"\"\",\n",
    "  backstory=\"\"\"You are a senior Embedded Linux Software engineer for Toradex, specialized in Linux kernel, U-boot, Firmware, Real-time, Yocto Project.\n",
    "  You possess in-depth knowledge of Toradex offerings, such as Torizon and the hardware families (Colibri, Apalis, and Verdin). \n",
    "  You have extensive experience in assessing technical responses and are accustomed to evaluating clarity and accuracy in technical communication within the field.\"\"\",\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  llm=ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0)\n",
    ")\n",
    "\n",
    "Customer_Reviewer = Agent(\n",
    "  role=\"Toradex customer evaluating and comparing AI and human responses on community forums\",\n",
    "  goal=\"\"\"Assess the AI-generated response against a corresponding human response from the community forum. Evaluate both responses based on how well they solve the problem presented, their clarity, directness, and relevance. Assign a score from 0 to 1, where 0 indicates that the AI response is completely divergent from the effective human response, and 1 indicates that the AI response is equivalent to or exceeds the human response in solving the query.\"\"\",\n",
    "  backstory=\"\"\"You are a Toradex customer who regularly uses products like the Apalis, Colibri, and Verdin modules and engages with the Torizon platform. You rely on the Toradex community forum for solving technical issues and are accustomed to evaluating the quality and effectiveness of the solutions provided.\"\"\",\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  llm=ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0)\n",
    ")\n",
    "\n",
    "Reviewer = Agent(\n",
    "  role=\"Toradex customer evaluating and comparing AI and human responses on community forums\",\n",
    "  goal=\"\"\"Consider both scores and return the average of the two values\"\"\",\n",
    "  backstory=\"\"\"You are a Toradex customer who regularly uses products like the Apalis, Colibri, and Verdin modules and engages with the Torizon platform. You rely on the Toradex community forum for solving technical issues and are accustomed to evaluating the quality and effectiveness of the solutions provided.\"\"\",\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  llm=ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0)\n",
    ")\n",
    "# Create tasks for your agents\n",
    "task1 = Task(\n",
    "  description=\"\"\"Evaluate the given AI-generated response to a technical query related to Embedded Linux development. \n",
    "  Consider factors such as clarity, completeness, accuracy, and relevance to the topic in your evaluation. \n",
    "  You will take into consideration the following aspects\n",
    "  1) Compare the AI response to what a well-informed human expert in Embedded Linux Software Engineering would provide.\n",
    "  2) Compare if the answers have simillar contents and solves the problem in simillar ways \n",
    "  3) The answer is clear and direct\n",
    "\n",
    "  Be extremaly critical and severe on your comparison!\n",
    "  \n",
    "  You will compare the answers:\n",
    "  Human answer: {human_answer}\n",
    "\n",
    "  AI answer: {ai_answer}\n",
    "\n",
    "  \"\"\",\n",
    "  expected_output=\"\"\"A numeric score ranging from 0 to 1, where 0 indicates that the AI-generated response is very different or unrelated compared to what a knowledgeable human expert would provide, \n",
    "  and 1 indicates that the AI-generated response is on par with a human expert in terms of clarity, completeness, accuracy, and relevance to Embedded Linux development.\n",
    "  The answer MUST be in the form below, having only a number between 0.0 and 1.0:\n",
    "  \n",
    "  [score]\n",
    "  \"\"\",\n",
    "  agent=Engineer_Reviewer,\n",
    "  #async_execution=True\n",
    ")\n",
    "\n",
    "task2 = Task(\n",
    "  description=\"\"\"Analyze and compare an AI-generated response with a human-provided response to a query posted on the community.toradex.com forum regarding an issue with using the Yocto Project on the Verdin development board. \n",
    "  Evaluate both responses based on clarity, directness, relevance, and problem-solving effectiveness. Consider how closely the AI-generated response aligns with the quality of the human response.\n",
    "  Be extremaly critical and severe on your comparison!\n",
    "  \n",
    "  You will compare the answers:\n",
    "  Human answer: {human_answer}\n",
    "\n",
    "  AI answer: {ai_answer}\n",
    "  \"\"\",\n",
    "  expected_output=\"\"\"A numeric score from 0 to 1, where 0 means the AI response is vastly inferior or unrelated compared to the human response, \n",
    "  and 1 means the AI response matches or surpasses the human response in addressing the query effectively. \n",
    "  The answer MUST be in the form below, having only a number between 0.0 and 1.0:\n",
    "  \n",
    "  [score]\n",
    "  \"\"\",\n",
    "  agent=Customer_Reviewer,\n",
    "  #async_execution=True\n",
    ")\n",
    "\n",
    "# Instantiate your crew with a sequential process\n",
    "crew = Crew(\n",
    "  agents=[Engineer_Reviewer, Customer_Reviewer],\n",
    "  tasks=[task1, task2],\n",
    "  verbose=False, # You can set it to 1 or 2 to different logging levels\n",
    "  full_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "res = range(0, 600)#random.sample(range(1, 599), 100)\n",
    "res = list(res)\n",
    "\n",
    "average_scores_list = []\n",
    "\n",
    "for index in res:\n",
    "    row = data_test[index]\n",
    "    inputs = {\n",
    "        'human_answer': row['answer'],\n",
    "        'ai_answer': row['generated_answer']\n",
    "    }\n",
    "    # Kickoff the crew with the correctly structured inputs\n",
    "    result = crew.kickoff(inputs=inputs)\n",
    "    # Extracting scores from the task outputs\n",
    "    scores = [float(task.exported_output) for task in result['tasks_outputs']]\n",
    "    \n",
    "    # Calculating the average score\n",
    "    average_score = round(sum(scores) / len(scores), 2)\n",
    "    average_scores_list.append(average_score)\n",
    "\n",
    "average_score = round(sum(average_scores_list) / len(average_scores_list), 2)\n",
    "print(\"Overall Average Score:\", average_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elrond-dataset-MHTID03v-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
