{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELROND Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urljoin\n",
    "from urllib.request import Request\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "from pathlib import Path  \n",
    "\n",
    "url_list = [\n",
    "    'https://developer.toradex.com',\n",
    "    'https://docs.u-boot.org/en/latest/index.html',\n",
    "    'https://docs.kernel.org',\n",
    "\t'https://docs.yoctoproject.org',\n",
    "\t'https://elinux.org/Main_Page',\n",
    "\t'https://wiki.archlinux.org/title/Table_of_contents',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_urls = set()\n",
    "internal_links = []\n",
    "max_depth = 10  # Set the maximum depth for recursion\n",
    "count = 0\n",
    "\n",
    "filepath = Path('../../dataset/checkpoint_links_list.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "def save_links(links, filename=filepath):\n",
    "    df = pd.DataFrame(links, columns=[\"URL\"])\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Saved {len(links)} links to {filename}\")\n",
    "\n",
    "# Function to extract and filter internal links\n",
    "def get_internal_links(base_url, soup):\n",
    "    internal_links = set()\n",
    "    \n",
    "    # General case for extracting internal links\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href')\n",
    "        if href.startswith('/'):\n",
    "            full_url = urljoin(base_url, href)\n",
    "            internal_links.add(full_url)\n",
    "        elif href.startswith(base_url):\n",
    "            internal_links.add(href)\n",
    "        elif href.startswith('.') or href.startswith('#'):\n",
    "            full_url = urljoin(base_url, href)\n",
    "            internal_links.add(full_url)\n",
    "    \n",
    "    # Specific case for toctree structures\n",
    "    for li in soup.find_all('li', class_=['toctree-l1', 'toctree-l2']):\n",
    "        for link in li.find_all('a', class_='reference internal'):\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                if href.startswith('/'):\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    internal_links.add(full_url)\n",
    "                elif href.startswith('http'):\n",
    "                    parsed_base_url = urlparse(base_url)\n",
    "                    parsed_href = urlparse(href)\n",
    "                    if parsed_href.netloc == parsed_base_url.netloc:\n",
    "                        internal_links.add(href)\n",
    "                else:\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    internal_links.add(full_url)\n",
    "\n",
    "    return internal_links\n",
    "\n",
    "def scrape_links(url, depth):\n",
    "    global count\n",
    "    if depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = get_internal_links(url, soup)\n",
    "\n",
    "            for link in links:\n",
    "                if link not in visited_urls:\n",
    "                    visited_urls.add(link)\n",
    "                    internal_links.append(link)\n",
    "                    count += 1\n",
    "\n",
    "                    if count % 100 == 0:\n",
    "                        save_links(internal_links)\n",
    "\n",
    "                    # Recursively scrape the link\n",
    "                    scrape_links(link, depth + 1)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage at {url}. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start scraping from the base URL\n",
    "for base_url in url_list:\n",
    "    scrape_links(base_url, 0)\n",
    "\n",
    "# Save remaining links if any\n",
    "if internal_links:\n",
    "    save_links(internal_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elrond-dataset-MHTID03v-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
